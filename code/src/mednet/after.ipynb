{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('../..')\n",
    "from src.utils.data import writePandas, getPandas, getConfig, getDict\n",
    "from src.model.feature import load_radiomics\n",
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AfterNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=512) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = 512\n",
    "        self.feature_size = 150\n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=-1)\n",
    "        self.train_qw = nn.Linear(4, hidden_dim)\n",
    "        nn.init.normal_(self.train_qw.weight, mean=0, std=0.01)\n",
    "        self.train_kw = nn.Linear(self.feature_size, hidden_dim)\n",
    "        nn.init.normal_(self.train_kw.weight, mean=0, std=0.01)\n",
    "        self.train_vw = nn.Linear(self.feature_size, hidden_dim)\n",
    "        nn.init.normal_(self.train_vw.weight, mean=0, std=0.01)\n",
    "        self.train_softmax = nn.Softmax(dim=2)\n",
    "        self.train_affine = nn.Linear(self.feature_size, 1)\n",
    "        self.flatten2 = nn.Flatten()\n",
    "        self.train_layernorm = nn.LayerNorm(512)\n",
    "        self.train_fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, tex, demo):\n",
    "        x = self.flatten(x)\n",
    "        res_x = x\n",
    "        q = self.train_qw(tex)\n",
    "        k = self.train_kw(x)\n",
    "        v = self.train_vw(x)\n",
    "        x = torch.bmm(q, k.transpose(1, 2))\n",
    "        x = x / math.sqrt(self.feature_size)\n",
    "        x = self.train_softmax(x)\n",
    "        x = torch.bmm(x, v)\n",
    "        x = self.flatten2(x)\n",
    "        res_x = self.train_affine(res_x)\n",
    "        res_x = res_x.transpose(1, 2)\n",
    "        res_x = self.flatten2(res_x)\n",
    "        x = x + res_x\n",
    "        x = self.train_layernorm(x)\n",
    "        x = self.train_fc(x)\n",
    "        return x\n",
    "    \n",
    "from torch.utils.data import dataset, dataloader\n",
    "class MyDataset(dataset.Dataset):\n",
    "    def __init__(self, keys, path, radiomics, cats, ages, sexs, scores, ledds, durations):\n",
    "        self.keys = keys\n",
    "        self.radiomics = radiomics\n",
    "        self.labels = cats\n",
    "        self.ages = ages\n",
    "        self.sexs = sexs\n",
    "        self.scores = scores\n",
    "        self.ledds = ledds\n",
    "        self.durations = durations\n",
    "        self.data = np.load(path)\n",
    "    def __getitem__(self, index):\n",
    "        key = self.keys[index]\n",
    "        radiomic = self.radiomics[index]\n",
    "        img = self.data[index]\n",
    "        tex = np.array(radiomic)\n",
    "        tex = np.expand_dims(tex, axis=0)\n",
    "        tex = torch.from_numpy(tex)\n",
    "        demo = np.array([self.ages[index], self.sexs[index], self.scores[index], self.ledds[index], self.durations[index]])\n",
    "        demo = torch.from_numpy(demo)\n",
    "        label = self.labels[index]\n",
    "        label = torch.from_numpy(np.array([label]))\n",
    "        return img, tex, demo, label\n",
    "    def __len__(self):\n",
    "        return len(self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getPandas('pat_data')\n",
    "conf = getConfig('data')\n",
    "train_idx = conf['indices']['pat']['train']\n",
    "test_idx = conf['indices']['pat']['test']\n",
    "keys = data['KEY'].values\n",
    "paths = data['ANTs_Reg'].values\n",
    "cats = data['CAT'].values\n",
    "radiomics = getPandas('pat_ANTs_Reg_radiomic')\n",
    "radiomics = radiomics.drop(['KEY'], axis=1)\n",
    "\n",
    "train_keys = keys[train_idx]\n",
    "train_path = 'data/bin/pat_resnet_10.npy'\n",
    "train_radiomics = radiomics.iloc[train_idx]\n",
    "radiomic_cols = [\n",
    "    'rTHA_original_gldm_LargeDependenceHighGrayLevelEmphasis',\n",
    "    'rTHA_original_glszm_LargeAreaHighGrayLevelEmphasis',\n",
    "    'rSN_original_glcm_ClusterProminence',\n",
    "    'rCAU_original_gldm_LargeDependenceHighGrayLevelEmphasis'\n",
    "]\n",
    "train_radiomics = train_radiomics[radiomic_cols].values\n",
    "radiomic_mean = train_radiomics.mean(axis=0)\n",
    "radiomic_std = train_radiomics.std(axis=0)\n",
    "train_radiomics = (train_radiomics - radiomic_mean) / radiomic_std\n",
    "train_cats = cats[train_idx]\n",
    "train_ages = data['AGE'].values[train_idx]/100\n",
    "train_sexs = data['SEX'].values[train_idx]\n",
    "train_scores = data['NUPDR3OF'].values[train_idx]\n",
    "train_ledds = data['LEDD'].values[train_idx]/100\n",
    "train_durations = data['DURATION'].values[train_idx]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optim_params = []\n",
    "\n",
    "data_set = MyDataset(train_keys, train_path, train_radiomics, train_cats, train_ages, train_sexs, train_scores, train_ledds, train_durations)\n",
    "data_loader = dataloader.DataLoader(data_set, batch_size=8, shuffle=True)\n",
    "epoch = 40\n",
    "from torcheval.metrics.aggregation.auc import AUC\n",
    "metric = AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rec = []\n",
    "lr_lsit = [1e-3]\n",
    "accumulation_steps = 8\n",
    "gq_list = []\n",
    "gv_list = []\n",
    "gk_list = []\n",
    "gl_list = []\n",
    "hidden_dim = 512\n",
    "for lr in lr_lsit:\n",
    "    loss_list = []\n",
    "    model = AfterNet(hidden_dim=hidden_dim)\n",
    "    kv_size = model.feature_size * hidden_dim\n",
    "    q_size = 4 * hidden_dim\n",
    "    model = model.cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'train' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    optimizer = optim.SGD([\n",
    "            {'params': model.train_qw.parameters(), 'lr': 1e-2},\n",
    "            {'params': model.train_kw.parameters(), 'lr': 1e-2},\n",
    "            {'params': model.train_vw.parameters(), 'lr': 1e-2},\n",
    "            {'params': model.train_fc.parameters(), 'lr': 1e-2},\n",
    "            {'params': model.train_affine.parameters(), 'lr': 1e-2},\n",
    "            {'params': model.train_layernorm.parameters(), 'lr': 1e-2}\n",
    "        ], lr=lr)\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=10, gamma=0.1)\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        for j, (img, tex, demo, label) in enumerate(data_loader):\n",
    "            #wq_pre = model.train_qw.weight\n",
    "            #wk_pre = model.train_kw.weight\n",
    "            #wv_pre = model.train_vw.weight\n",
    "            img = img.cuda().float()\n",
    "            tex = tex.cuda().float()\n",
    "            demo = demo.cuda().float()\n",
    "            label = label.cuda().float()\n",
    "            out = model(img, tex, demo)\n",
    "            loss = loss_fn(out, label)\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            if (j + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                wq = model.train_qw.weight\n",
    "                wk = model.train_kw.weight\n",
    "                wv = model.train_vw.weight\n",
    "                wl = model.train_fc.weight\n",
    "                gq = wq.grad / q_size\n",
    "                gk = wk.grad / kv_size\n",
    "                gv = wv.grad / kv_size \n",
    "                gl = wl.grad / hidden_dim\n",
    "                gq_sum = np.sum(np.abs(gq.cpu().detach().numpy()))\n",
    "                gk_sum = np.sum(np.abs(gk.cpu().detach().numpy()))\n",
    "                gv_sum = np.sum(np.abs(gv.cpu().detach().numpy()))\n",
    "                gl_sum = np.sum(np.abs(gl.cpu().detach().numpy()))\n",
    "                print('gq: {}, gk: {}, gv: {}, gl: {}'.format(gq_sum, gk_sum, gv_sum, gl_sum))\n",
    "                gq_list.append(gq_sum)\n",
    "                gk_list.append(gk_sum)\n",
    "                gv_list.append(gv_sum)\n",
    "                gl_list.append(gl_sum)\n",
    "                if i > 1:\n",
    "\n",
    "                    sns.lineplot(gq_list[1:])\n",
    "                    plt.title('gq')\n",
    "                    plt.show()\n",
    "\n",
    "                    sns.lineplot(gk_list[1:])\n",
    "                    plt.title('gk')\n",
    "                    plt.show()\n",
    "\n",
    "                    sns.lineplot(gv_list[1:])\n",
    "                    plt.title('gv')\n",
    "                    plt.show()\n",
    "\n",
    "                    sns.lineplot(gl_list[1:])\n",
    "                    plt.title('gl')\n",
    "                    plt.show()\n",
    "                optimizer.zero_grad()\n",
    "            #optimizer.zero_grad()\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "        #print('epoch: {}, loss: {}'.format(i, loss.item()))\n",
    "        #scheduler.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            total_loss = 0\n",
    "            metric.reset()\n",
    "            output = np.array([])\n",
    "            for img, tex, demo, label in data_loader:\n",
    "                img = img.cuda().float()\n",
    "                tex = tex.cuda().float()\n",
    "                demo = demo.cuda().float()\n",
    "                label = label.cuda().float()\n",
    "                out = model(img, tex, demo)\n",
    "                loss = loss_fn(out, label)\n",
    "                total_loss += loss.item()\n",
    "                #print(torch.squeeze(out).cpu().detach().numpy(), torch.squeeze(label).cpu().detach().numpy())\n",
    "                output = np.append(output, torch.squeeze(out).cpu().detach().numpy())\n",
    "                out = torch.sigmoid(out)\n",
    "                metric.update(torch.squeeze(out), torch.squeeze(label))\n",
    "                out = torch.round(out)\n",
    "                out = out.cpu().detach().numpy()\n",
    "                total += label.size(0)\n",
    "                correct += (out == label.cpu().detach().numpy()).sum().item()\n",
    "            sns.histplot(output)\n",
    "            plt.show()\n",
    "            loss_item = total_loss / len(train_ages)\n",
    "            loss_list.append(loss_item)\n",
    "            print('epoch: {}, loss: {}'.format(i, total_loss / len(data_loader)))\n",
    "            print('acc: {}'.format(correct / total))\n",
    "            print('auc: {}'.format(metric.compute().cpu().detach().numpy()))\n",
    "    loss_rec.append(loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
